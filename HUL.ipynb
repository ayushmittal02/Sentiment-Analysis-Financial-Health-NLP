{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis of the Financial Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module uses Annual Report of 'HDFC', 'Hindustan Unilever', 'Infosys', 'Kotak', 'Reliance' and 'TCS' that are publicly listed companies and analyses the management's discussion section. This section of the report if written with a negative sentiment shows the performance is going down. The dictionary used is Loughran-McDonald words dictionary because the standard NLTK dictionary does not hold true in case of finance due to different contextual meaning.\n",
    "\n",
    "The process is as follows:\n",
    "1. Read the MD&A from an annual report and tokenize the same\n",
    "2. Remove stop words such as and, of, the, etc.\n",
    "3. Importing Loughran-McDonald dictionaries and separating the words into positive and negative words\n",
    "4. Calculating overall sentiment score\n",
    "\n",
    "The analysis can be easily reproduced for other firms to check their sentiment using this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindustan Unilever (HUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the HUL 2015-16 report are 1075\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HUL in 2015-16\n",
    "'''\n",
    "\n",
    "with open (\"hul_2015_16.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    tokens_old = nltk.word_tokenize(f.read())\n",
    "    \n",
    "no_digit = re.compile('.*[A-Za-z].*')\n",
    "tokens = [w for w in tokens_old if no_digit.match(w)]\n",
    "tokens = list(map(lambda x: x.lower(),tokens))\n",
    "print(\"Total words in the HUL 2015-16 report are {0}\".format(len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words are ['the', 'of', 'and', 'to', 'in', 'our', 'with', 'we', 'a', 'by']\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(tokens)\n",
    "stop = []\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    stop.append(word)\n",
    "print(\"Stop words are {0}\".format(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in tokens:\n",
    "    if word in stop:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The stopwords do not have 'no', 'not' and 'never' in them\n",
    "So, directly removing these words in the next step\n",
    "'''\n",
    "final_tokens = [x for x in tokens if x not in stop]\n",
    "total_tokens = len(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total positive words are 43\n",
      "Positive words are: \n",
      "['great', 'pleasure', 'profitable', 'innovation', 'efficient', 'efficient', 'profitable', 'impressive', 'improving', 'despite', 'improvement', 'exceptional', 'exceptional', 'strong', 'strong', 'proactively', 'strengthened', 'strong', 'gains', 'improved', 'successfully', 'exciting', 'strengthen', 'strong', 'leading', 'strong', 'strengthened', 'strengths', 'efficiencies', 'strengthen', 'improvement', 'leading', 'innovative', 'enabling', 'good', 'achieved', 'successfully', 'pleased', 'premier', 'progress', 'improving', 'opportunity', 'successful']\n",
      "\n",
      "Total negative words are 9\n",
      "Negative words are: \n",
      "['challenging', 'failure', 'drought', 'distressed', 'negative', 'adverse', 'against', 'challenge', 'shortage']\n"
     ]
    }
   ],
   "source": [
    "# Source: https://sraf.nd.edu/textual-analysis/resources/\n",
    "# This source provides Loughran-McDonald Sentiment word lists for finance\n",
    "lm_neg = open(\"LM_neg_words.txt\").read().lower()\n",
    "lm_pos = open(\"LM_pos_words.txt\").read().lower()\n",
    "\n",
    "li_neg = lm_neg.split('\\n')\n",
    "li_pos = lm_pos.split('\\n')\n",
    "\n",
    "checking = {}\n",
    "checking['positive'] = li_pos\n",
    "checking['negative'] = li_neg\n",
    "\n",
    "result_pos = []\n",
    "result_neg = []\n",
    "for word in final_tokens:\n",
    "    if word in checking['positive']:\n",
    "        result_pos.append(word)\n",
    "    elif word in checking['negative']:\n",
    "        result_neg.append(word)\n",
    "\n",
    "print()\n",
    "print(\"Total positive words are {0}\".format(len(result_pos)))\n",
    "print('Positive words are: ')\n",
    "print(result_pos)\n",
    "print()\n",
    "print(\"Total negative words are {0}\".format(len(result_neg)))\n",
    "print('Negative words are: ')\n",
    "print(result_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HUL's total sentiment in 2015-16 is : 0.04342273307790549\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Calculating\n",
    "(positive words - negative words)/total words\n",
    "'''\n",
    "\n",
    "pos = len(result_pos)\n",
    "neg = len(result_neg)\n",
    "\n",
    "sentiment = (pos-neg)/total_tokens\n",
    "print()\n",
    "print(\"HUL's total sentiment in 2015-16 is : {0}\".format(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the HUL 2016-17 report are 1159\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HUL 2016-17\n",
    "'''\n",
    "\n",
    "with open (\"hul_2016_17.txt\", encoding=\"utf8\", errors='ignore') as f2:\n",
    "    tokens_old = nltk.word_tokenize(f2.read())\n",
    "    \n",
    "no_digit2 = re.compile('.*[A-Za-z].*')\n",
    "tokens2 = [w for w in tokens_old if no_digit2.match(w)]\n",
    "tokens2 = list(map(lambda x: x.lower(),tokens2))\n",
    "print(\"Total words in the HUL 2016-17 report are {0}\".format(len(tokens2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words are ['the', 'and', 'of', 'in', 'our', 'to', 'we', 'by', 'a', 'with']\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(tokens2)\n",
    "stop2 = []\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    stop2.append(word)\n",
    "print(\"Stop words are {0}\".format(stop2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in tokens2:\n",
    "    if word in stop2:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The stopwords do not have 'no', 'not' and 'never' in them\n",
    "So, directly removing these words in the next step\n",
    "'''\n",
    "final_tokens2 = [x for x in tokens2 if x not in stop2]\n",
    "total_tokens2 = len(final_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total positive words are 30\n",
      "Positive words are: \n",
      "['great', 'pleasure', 'enabled', 'profitable', 'good', 'improvement', 'exceptional', 'strong', 'strong', 'strong', 'strengthened', 'strong', 'innovation', 'strengths', 'improving', 'winning', 'advantage', 'improve', 'enabling', 'better', 'excellent', 'excellence', 'good', 'innovative', 'achievements', 'empower', 'efficient', 'pleased', 'premier', 'opportunity']\n",
      "\n",
      "Total negative words are 5\n",
      "Negative words are: \n",
      "['difficult', 'sluggish', 'poor', 'slowdown', 'challenging']\n"
     ]
    }
   ],
   "source": [
    "lm_neg2 = open(\"LM_neg_words.txt\").read().lower()\n",
    "lm_pos2 = open(\"LM_pos_words.txt\").read().lower()\n",
    "\n",
    "li_neg2 = lm_neg2.split('\\n')\n",
    "li_pos2 = lm_pos2.split('\\n')\n",
    "\n",
    "checking2 = {}\n",
    "checking2['positive'] = li_pos2\n",
    "checking2['negative'] = li_neg2\n",
    "\n",
    "result_pos2 = []\n",
    "result_neg2 = []\n",
    "for word in final_tokens2:\n",
    "    if word in checking2['positive']:\n",
    "        result_pos2.append(word)\n",
    "    elif word in checking2['negative']:\n",
    "        result_neg2.append(word)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Total positive words are {0}\".format(len(result_pos2)))\n",
    "print('Positive words are: ')\n",
    "print(result_pos2)\n",
    "print()\n",
    "print(\"Total negative words are {0}\".format(len(result_neg2)))\n",
    "print('Negative words are: ')\n",
    "print(result_neg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HUL's total sentiment in 2016-17 was : 0.02997601918465228\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Calculating\n",
    "(positive words - negative words)/total words\n",
    "'''\n",
    "\n",
    "pos2 = len(result_pos2)\n",
    "neg2 = len(result_neg2)\n",
    "\n",
    "sentiment2 = (pos2-neg2)/total_tokens2\n",
    "print()\n",
    "print(\"HUL's total sentiment in 2016-17 was : {0}\".format(sentiment2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the HUL 2017-18 report are 1087\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HUL 2017-18\n",
    "'''\n",
    "with open (\"hul_2017_18.txt\", encoding=\"utf8\", errors='ignore') as f3:\n",
    "    tokens_old = nltk.word_tokenize(f3.read())\n",
    "    \n",
    "no_digit3 = re.compile('.*[A-Za-z].*')\n",
    "tokens3 = [w for w in tokens_old if no_digit3.match(w)]\n",
    "tokens3 = list(map(lambda x: x.lower(),tokens3))\n",
    "print(\"Total words in the HUL 2017-18 report are {0}\".format(len(tokens3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words are ['and', 'the', 'to', 'of', 'in', 'our', 'a', 'with', 'we', 'growth']\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(tokens3)\n",
    "stop3 = []\n",
    "for word, frequency in fdist.most_common(10):\n",
    "    stop3.append(word)\n",
    "print(\"Stop words are {0}\".format(stop3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in tokens3:\n",
    "    if word in stop3:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The stopwords do not have 'no', 'not' and 'never' in them\n",
    "So, directly removing these words in the next step\n",
    "'''\n",
    "final_tokens3 = [x for x in tokens3 if x not in stop3]\n",
    "total_tokens3 = len(final_tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total positive words are 44\n",
      "Positive words are: \n",
      "['great', 'pleasure', 'benefit', 'improvement', 'strong', 'winning', 'profitable', 'improvement', 'exceptional', 'strong', 'strong', 'strengthened', 'outstanding', 'successful', 'innovate', 'excellent', 'strong', 'strengthen', 'innovations', 'strong', 'innovations', 'excellent', 'strong', 'innovations', 'leadership', 'innovative', 'boosted', 'advantage', 'enhance', 'win', 'progress', 'improvement', 'positive', 'benefitted', 'improving', 'collaborating', 'achievements', 'leading', 'good', 'great', 'pleasure', 'confident', 'leadership', 'greater']\n",
      "\n",
      "Total negative words are 4\n",
      "Negative words are: \n",
      "['volatile', 'challenging', 'challenges', 'depleting']\n"
     ]
    }
   ],
   "source": [
    "lm_neg3 = open(\"LM_neg_words.txt\").read().lower()\n",
    "lm_pos3 = open(\"LM_pos_words.txt\").read().lower()\n",
    "\n",
    "li_neg3 = lm_neg3.split('\\n')\n",
    "li_pos3 = lm_pos3.split('\\n')\n",
    "\n",
    "checking3 = {}\n",
    "checking3['positive'] = li_pos3\n",
    "checking3['negative'] = li_neg3\n",
    "\n",
    "result_pos3 = []\n",
    "result_neg3 = []\n",
    "for word in final_tokens3:\n",
    "    if word in checking3['positive']:\n",
    "        result_pos3.append(word)\n",
    "    elif word in checking3['negative']:\n",
    "        result_neg3.append(word)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Total positive words are {0}\".format(len(result_pos3)))\n",
    "print('Positive words are: ')\n",
    "print(result_pos3)\n",
    "print()\n",
    "print(\"Total negative words are {0}\".format(len(result_neg3)))\n",
    "print('Negative words are: ')\n",
    "print(result_neg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HUL's total sentiment in 2017-18 was : 0.04987531172069826\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Calculating\n",
    "(positive words - negative words)/total words\n",
    "'''\n",
    "\n",
    "pos3 = len(result_pos3)\n",
    "neg3 = len(result_neg3)\n",
    "\n",
    "sentiment3 = (pos3-neg3)/total_tokens3\n",
    "print()\n",
    "print(\"HUL's total sentiment in 2017-18 was : {0}\".format(sentiment3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
